{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # K–Means Seeded Model Testing\n",
    "# This notebook loads a pre-trained seeded K–Means model along with the corresponding scaler and PCA clustering model.\n",
    "# It then processes a specified fraction of a new dataset, applies the same feature transformations,\n",
    "# predicts cluster labels, computes evaluation metrics (e.g., silhouette score, confusion matrix, classification report),\n",
    "# and saves the evaluation outputs.\n",
    "\n",
    "# %% [code]\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, \n",
    "                             silhouette_score)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Parameter Definition\n",
    "# Adjust these parameters as needed.\n",
    "dataset_path = r\"C:\\Users\\mrroo\\Desktop\\RP3\\datasets\\2018.csv\"  # Update path as needed\n",
    "sample_fraction = 0.03  # e.g., use 3% of the data for testing\n",
    "run_name = \"kmeans_seeded_test_run_full\"  # Change as desired\n",
    "year = \"2018\"\n",
    "model_name = \"KMeans_Seeded\"\n",
    "\n",
    "# Define the output folder for this test run\n",
    "output_folder = os.path.join(\"outputs\", year, model_name, run_name)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(\"Output folder created:\", output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Load Saved Model Artifacts\n",
    "# Update the following paths to point to your saved artifacts.\n",
    "seeded_model_path = r\"C:\\Users\\mrroo\\Desktop\\RP3\\Imp\\Outputs\\2018\\k-means\\run1-20%\\seeded_kmeans_model.pkl\"  # Update path\n",
    "pca_model_path = r\"C:\\Users\\mrroo\\Desktop\\RP3\\Imp\\Outputs\\2018\\k-means\\run1-20%\\pca_clustering_model.pkl\"  # Update path\n",
    "scaler_path = r\"C:\\Users\\mrroo\\Desktop\\RP3\\Imp\\Outputs\\2018\\k-means\\run1-20%\\scaler.pkl\"  # Update path\n",
    "\n",
    "seeded_kmeans_model = joblib.load(seeded_model_path)\n",
    "pca_clustering = joblib.load(pca_model_path)\n",
    "scaler_loaded = joblib.load(scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Data Loading & Preprocessing\n",
    "data = pd.read_csv(dataset_path)\n",
    "print(\"Original dataset shape:\", data.shape)\n",
    "print(\"First few rows:\")\n",
    "print(data.head())\n",
    "print(\"\\nDataset info:\")\n",
    "print(data.info())\n",
    "if 'label' in data.columns:\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(data['label'].value_counts())\n",
    "\n",
    "# Sample a fraction of the dataset for testing\n",
    "data = data.sample(frac=sample_fraction, random_state=42).reset_index(drop=True)\n",
    "print(f\"Dataset shape after sampling {sample_fraction*100:.0f}%:\", data.shape)\n",
    "\n",
    "# Remove misread header rows (if any)\n",
    "data = data[data['label'] != 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Encode Labels (if available)\n",
    "if 'label' in data.columns:\n",
    "    data['label'], uniques = pd.factorize(data['label'])\n",
    "    label_names = {i: label for i, label in enumerate(uniques)}\n",
    "    print(\"\\nEncoded labels mapping:\")\n",
    "    print(label_names)\n",
    "else:\n",
    "    print(\"No 'label' column found; skipping label encoding.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Feature Selection\n",
    "# Option 1: Manually define features (if desired)\n",
    "manual_features = ['pkts_ratio', 'Tot Bwd Pkts', 'Dst Port', 'Fwd Pkt Len Max',\n",
    "                   'SYN Flag Cnt', 'byte_per_duration', 'Bwd Pkt Len Min', 'protocol',\n",
    "                   'Fwd Pkt Len Mean', 'TotLen Bwd Pkts', 'Fwd Pkt Len Min', 'Flow Duration',\n",
    "                   'Flow Duration_rolling_std', 'Flow Duration_rolling_mean', 'Bwd Pkt Len Max',\n",
    "                   'Fwd IAT Tot', 'TotLen Fwd Pkts', 'Subflow Fwd Byts', 'Fwd IAT Max',\n",
    "                   'Tot Fwd Pkts', 'Fwd IAT Min', 'Fwd IAT Mean', 'Fwd Pkt Len Std',\n",
    "                   'entropy_pkt_len']\n",
    "\n",
    "try:\n",
    "    selected_features = manual_features\n",
    "except NameError:\n",
    "    selected_features = [col for col in data.columns if col.lower() not in ['timestamp', 'label']]\n",
    "\n",
    "print(\"\\nSelected features:\")\n",
    "print(selected_features)\n",
    "\n",
    "X = data[selected_features]\n",
    "if 'label' in data.columns:\n",
    "    y = data['label']\n",
    "\n",
    "# Drop rows with missing feature values\n",
    "num_rows_before = X.shape[0]\n",
    "X = X.dropna()\n",
    "num_rows_after = X.shape[0]\n",
    "print(\"Number of rows dropped due to missing values:\", num_rows_before - num_rows_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Standardize Features\n",
    "# IMPORTANT: Ensure the order of features matches what was used during training.\n",
    "if hasattr(scaler_loaded, \"feature_names_in_\"):\n",
    "    correct_feature_order = list(scaler_loaded.feature_names_in_)\n",
    "    print(\"Feature order from training:\", correct_feature_order)\n",
    "    X = X[correct_feature_order]\n",
    "else:\n",
    "    X = X[selected_features]\n",
    "\n",
    "X_scaled = scaler_loaded.transform(X)\n",
    "print(\"Scaled features shape:\", X_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Apply PCA Transformation\n",
    "# Transform the test data using the saved PCA clustering model.\n",
    "X_pca_test = pca_clustering.transform(X_scaled)\n",
    "print(\"Transformed test features shape:\", X_pca_test.shape)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Model Evaluation\n",
    "# Predict cluster labels using the loaded seeded K–Means model.\n",
    "predicted_clusters = seeded_kmeans_model.predict(X_pca_test)\n",
    "print(\"Predicted cluster labels shape:\", predicted_clusters.shape)\n",
    "\n",
    "# Compute silhouette score on PCA-transformed test data.\n",
    "sil_score = silhouette_score(X_pca_test, predicted_clusters)\n",
    "print(\"Silhouette Score:\", sil_score)\n",
    "\n",
    "# If ground truth labels are available, compute confusion matrix and classification report.\n",
    "if 'label' in data.columns:\n",
    "    y_true = y  # Ground truth labels from test data\n",
    "    cm = confusion_matrix(y_true, predicted_clusters)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    unique_classes = np.union1d(np.unique(y_true), np.unique(predicted_clusters))\n",
    "    target_names_test = [str(cls) for cls in unique_classes]\n",
    "    \n",
    "    report = classification_report(\n",
    "        y_true,\n",
    "        predicted_clusters,\n",
    "        labels=unique_classes,\n",
    "        target_names=target_names_test,\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "else:\n",
    "    print(\"No ground truth labels available; skipping confusion matrix and classification report.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### Plot Confusion Matrix (if ground truth is available)\n",
    "if 'label' in data.columns:\n",
    "    def plot_confusion_matrix(cm, classes, title=\"Confusion Matrix\"):\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        ax.set_xticks(tick_marks)\n",
    "        ax.set_xticklabels(classes, rotation=45, fontsize=12)\n",
    "        ax.set_yticks(tick_marks)\n",
    "        ax.set_yticklabels(classes, fontsize=12)\n",
    "        ax.set_xlabel(\"Predicted label\", fontsize=14)\n",
    "        ax.set_ylabel(\"True label\", fontsize=14)\n",
    "        ax.set_title(title, fontsize=16)\n",
    "        thresh = cm.max() / 2.0\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, format(cm[i, j], \"d\"),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                        fontsize=12)\n",
    "        fig.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    fig_cm = plot_confusion_matrix(cm, target_names_test)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping confusion matrix plot as no ground truth labels are available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Save Evaluation Outputs\n",
    "results_txt_path = os.path.join(output_folder, \"evaluation_report.txt\")\n",
    "with open(results_txt_path, \"w\") as f:\n",
    "    f.write(\"Evaluation Metrics\\n\")\n",
    "    f.write(\"==================\\n\\n\")\n",
    "    f.write(\"Silhouette Score: {:.4f}\\n\\n\".format(sil_score))\n",
    "    if 'label' in data.columns:\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(report)\n",
    "        f.write(\"\\nConfusion Matrix:\\n\")\n",
    "        f.write(np.array2string(cm, separator=\", \"))\n",
    "print(\"Evaluation report saved to:\", results_txt_path)\n",
    "\n",
    "if 'label' in data.columns:\n",
    "    cm_df = pd.DataFrame(cm, index=target_names_test, columns=target_names_test)\n",
    "    cm_csv_path = os.path.join(output_folder, \"confusion_matrix.csv\")\n",
    "    cm_df.to_csv(cm_csv_path, index=True)\n",
    "    print(\"Confusion matrix saved as CSV:\", cm_csv_path)\n",
    "\n",
    "sil_score_path = os.path.join(output_folder, \"silhouette_score.txt\")\n",
    "with open(sil_score_path, \"w\") as f:\n",
    "    f.write(\"Silhouette Score: {:.4f}\\n\".format(sil_score))\n",
    "print(\"Silhouette score saved to:\", sil_score_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
